# -*- coding: utf-8 -*-
"""XGB_Class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XgMn7Iic1KcT0qjoBlL5UNYGYUB29XuO
"""

import numpy as np
import pandas as pd
import sklearn.datasets
import sklearn.metrics
from sklearn.model_selection import train_test_split
import xgboost as xgb
import optuna
from hyperopt import hp, fmin, tpe, Trials, STATUS_OK
from hyperopt.pyll.stochastic import sample
from sklearn.metrics import log_loss
from timeit import default_timer as timer
import random



class XGB_Higgs():

  """
  Executes methods that split a dataset into training and testing sets, runs an 
  XGBoost classifier with cross-validation, and attempts to optimize the 
  hyperparameters using three different methods: HyperOpt, Optuna and 
  RandomSearch.
  """

  def __init__(self, filename, max_evals, n_fold):

    """
    Initializes an instance of the XGB_Higgs Class

    Parameters
    ----------
    filename: string
      A path containing the data file to be explored
    max_evals: int
      The number of trials to run the optimization algorithms
    n_fold: int
      Number of cross-validation folds to use during validation

    """

    self.filename = filename
    self.data = pd.read_csv(self.filename)
    self.data = self.data.drop(['Unnamed: 0'],axis=1)
    self.max_evals = max_evals
    self.nfold = n_fold
    self.x_train, self.x_test = None, None
    self.y_train, self.y_test = None, None
    self.logloss = None
    self.param = None

  def split_data(self):
    
    """
    Splits the data into a training and testing test

    """
    x = self.data.iloc[:, 1:29]
    y = self.data.iloc[:, 0]
    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size = 0.3)
    self.dtrain = xgb.DMatrix(self.x_train, label = self.y_train)
    self.dtest = xgb.DMatrix(self.x_test, label = self.y_test)

  def hyperopt_tuning(self):

    """
    Applies the HyperOpt algorithm to tune the hyperparameters of the XGBoost
    model and stores the results in a dataframe

    Returns
    ----------
    best: dict
      A dictionary containing the set of parameters that resulted in the best
      value of the loss function using the HyperOpt optimization algorithm.
    """

    self.hyperopt_results = pd.DataFrame(columns = ['params', 'loss', 'variance', 'time'])

    def hyperopt_params():
      
      """
      Defines the hyperparameter search space for the HyperOpt algorithm

      Returns
      ----------
      params: dict
        A dictionary containing the search-space for the Hyperopt optimization 
        algorithm
      """

      params = {
        "silent": 1,
        "objective": "binary:logistic",
        "eval_metric": "logloss",
        "booster": hp.choice("booster", ["gbtree", "dart"]),
        "reg_lambda": hp.quniform("reg_lambda", 1, 2, 0.1),
        "reg_alpha": hp.quniform("reg_alpha", 0, 10, 1),
        "verbosity": 2,
        "n_estimators": hp.choice("n_estimators", np.arange(50, 510, 50, dtype = int)),
        "max_depth": hp.choice("max_depth", np.arange(1, 14, dtype = int)),
        "eta": hp.quniform('eta', 0.025, 0.5, 0.025),
        "gamma": hp.quniform("gamma", 0.5, 1.0, 0.05),
        "grow_policy": hp.choice("grow_policy", ["depthwise", "lossguide"]),
        "min_child_weight": hp.quniform('min_child_weight', 1, 10, 1),
        "subsample": hp.quniform("subsample", 0.7, 1, 0.05),
        "colsample_bytree": hp.quniform("colsample_bytree", 0.7, 1, 0.05),
        "max_leaves": hp.choice("max_leaves", np.arange(0, 10, 1, dtype = int)),
        "sample_type": hp.choice("sample_type", ["uniform", "weighted"]),
        "normalize_type": hp.choice("normalize_type", ["tree", "forest"]),
        "rate_drop": hp.uniform("rate_drop", 0, 1),
        "skip_drop": hp.uniform("skip_drop", 0, 1)
        }
      return params
      
    space = hyperopt_params()

    def hyperopt_objective(space):

      """
      Defines the objective function to be optimized by the HyperOpt algorithm

      Parameters
      ----------
      space: dict
        A dictionary of parameters to use for each trial of the HyperOpt
        optimization algorithm

      Returns
      ----------
      result_dict: dict
        A dictionary containing the loss for the current set of parameters, 
        the status of the current trial and a dictionary of parameters used
        for the trial
      """


      print("Training with params: ")
      print(space)
      start = timer()
      cv_results = xgb.cv(space, 
                        self.dtrain,
                        num_boost_round = space["n_estimators"],
                        verbose_eval = True,
                        nfold = self.nfold,
                        metrics = space["eval_metric"])
      end = timer()
      cv_score = np.min(cv_results['test-logloss-mean'])
      cv_var = np.min(cv_results['test-logloss-std'])**2

      self.hyperopt_results = self.hyperopt_results.append({'params': space, 'loss': cv_score, 'variance': cv_var, 'time': end - start}, ignore_index = True)
      
      result_dict = {'loss': cv_score, 'status': STATUS_OK, 'parameters': space}

      return (result_dict)


    trials = Trials()
    best = fmin(hyperopt_objective, space, algo = tpe.suggest, 
                trials = trials, 
                max_evals = self.max_evals)
    print("The best hyperparameters are: ", "\n")
    print(best)
    return best

  def optuna_tuning(self):

    """
    Applies the Optuna algorithm to tune the hyperparameters of the XGBoost
    model and stores the results in a dataframe

    Returns
    ----------
    best: dict
      A dictionary containing the set of parameters that resulted in the best
      value of the loss function using the Optuna optimization algorithm.
    """
    self.optuna_results = pd.DataFrame(columns = ['params', 'loss', 'variance', 'time'])
    
    def optuna_objective(trial):

      """
      Defines the objective function to be optimized by the Optuna algorithm

      Parameters
      ----------
      trial: Optuna trial object
        An Optuna trial that iterates over a set of hyperparameters to 
        minimize the objective function

      Returns
      ----------
      cv_score: float
        The value of the loss function cross-validated over n-folds for the
        current set of hyperparameters for the trial
      """

      space = {
        "silent": 1,
        "objective": "binary:logistic",
        "eval_metric": "logloss",
        "verbosity": 2,
        "booster": trial.suggest_categorical("booster", ["gbtree", "dart"]),
        "reg_lambda": trial.suggest_int("reg_lambda", 1, 2),
        "reg_alpha": trial.suggest_int("reg_alpha", 0, 10),
        "n_estimators": trial.suggest_int("n_estimators", 50, 510),
        "max_depth": trial.suggest_int("max_depth", 1, 14),
        "eta": trial.suggest_uniform("eta", 0.025, 0.5),
        "gamma": trial.suggest_uniform("gamma", 0.5, 1.0),
        "grow_policy": trial.suggest_categorical("grow_policy", ["depthwise", "lossguide"]),
        "min_child_weight": trial.suggest_uniform("min_child_weight", 1, 10),
        "subsample": trial.suggest_uniform("subsample", 0.7, 1),
        "colsample_bytree": trial.suggest_uniform("colsample_bytree", 0.7, 1),
        "max_leaves": trial.suggest_int("max_leaves", 0, 10),
        "sample_type": trial.suggest_categorical("sample_type", ["uniform", "weighted"]),
        "normalize_type": trial.suggest_categorical("normalize_type", ["tree", "forest"]),
        "rate_drop": trial.suggest_uniform("rate_drop", 0, 1),
        "skip_drop": trial.suggest_uniform("skip_drop", 0, 1)
        }

      # pruning_callback = optuna.integration.XGBoostPruningCallback(trial, "test-logloss")
      start = timer()
      cv_results = xgb.cv(space, 
                        self.dtrain,
                        num_boost_round = 5,
                        verbose_eval = True,
                        nfold = self.nfold,
                        metrics = space["eval_metric"]
                        # callbacks = [pruning_callback]
                        )
      end = timer()

      cv_score = np.min(cv_results['test-logloss-mean'])
      cv_var = np.min(cv_results['test-logloss-std'])**2
      cv_params = space

      self.optuna_results = self.optuna_results.append({'params': cv_params, 'loss': cv_score, 'variance': cv_var, 'time': end - start}, ignore_index = True)

      return cv_score

    study = optuna.create_study(pruner = optuna.pruners.MedianPruner(n_warmup_steps = 5), direction = "minimize")
    best = study.optimize(optuna_objective, n_trials = 100)

    print("The best hyperparameters are: ", "\n")
    print(best)
    return best

  def random_search_tuning(self):

    """
    Applies the Random Search algorithm to tune the hyperparameters of the 
    XGBoost model and stores the results in a dataframe

    Returns
    ----------
    best: dict
      A dictionary containing the set of parameters that resulted in the best
      value of the loss function using the Random Search optimization algorithm.
    """

    self.random_search_results = pd.DataFrame(columns = ['params', 'loss', 'variance', 'time'])

    def random_params():

      """
      Defines the hyperparameter search space for the Random Search algorithm

      Returns
      ----------
      params: dict
        A dictionary containing the search-space for the Hyperopt optimization 
        algorithm
      """

      params = {
          "silent": [1],
          "objective": ["binary:logistic"],
          "eval_metric": ["logloss"],
          "booster": ["gbtree", "dart"],
          "reg_lambda": np.arange(1, 2, 0.1),
          "reg_alpha": np.arange(0, 10, 1),
          "verbosity": [2],
          "n_estimators": np.arange(50, 510, 50),
          "max_depth": np.arange(1, 14),
          "eta": np.arange(0.025, 0.5, 0.025),
          "gamma": np.arange(0.5, 1.0, 0.05),
          "grow_policy": ["depthwise", "lossguide"],
          "min_child_weight": np.arange(1, 10, 1),
          "subsample": np.arange(0.7, 1, 0.05),
          "colsample_bytree": np.arange(0.7, 1, 0.05),
          "max_leaves": np.arange(0, 10, 1),
          "sample_type": ["uniform", "weighted"],
          "normalize_type": ["tree", "forest"],
          "rate_drop": np.linspace(0, 1),
          "skip_drop": np.linspace(0, 1)
          }
      return params

    space = random_params()

    def random_objective(space):

      """
      Defines the objective function to be optimized by the Random Search
      algorithm

      Parameters
      ----------
      space: dict
        A dictionary of parameters to use for each trial of the Random Search
        optimization algorithm

      """

      print("Training with params: ")
      print(space)

      start = timer()

      cv_results = xgb.cv(space, 
                        self.dtrain,
                        num_boost_round = space['n_estimators'],
                        verbose_eval = True,
                        nfold = self.nfold,
                        metrics = space["eval_metric"]
                        )
      end = timer()

      cv_score = np.min(cv_results['test-logloss-mean'])
      cv_var = np.min(cv_results['test-logloss-std'])**2
      cv_params = space

      self.random_search_results = self.random_search_results.append({'params': cv_params, 'loss': cv_score, 'variance': cv_var, 'time': end - start}, ignore_index = True)

    for i in range(self.max_evals):

      param_sample = {key: random.sample(list(value), 1)[0] for key, value in space.items()}

      random_objective(param_sample)

    best = self.random_search_results[['params', 'loss']].sort_values(by = 'loss', ascending = True).loc[0].to_dict()

    print("The best hyperparameters are: ")
    print(best)

    return best